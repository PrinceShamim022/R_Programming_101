---
title: "Binary Classification of Diabetes & Visualization"
author: "Abdullah Al Shamim"
date: "September 30, 2023"
output: 
    html_document:
        number_sections: true
        fig_caption: true
        toc: true
        fig_width: 12
        fig_height: 8
        theme: cosmo
        highlight: tango
        code_folding: hide
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(warning=FALSE, message=FALSE)

```

# Data Importing & Cleaning & Inspecting

## Import dataset

variable name 'diab' is abbreviation of 'diabetes'

```{r}

diab <- read.csv("diabetes.csv", header=T, stringsAsFactors=F)

```

## Inspect dataset

```{r}

summary(diab)

```

## Rename Variables

For better explanation, change the column name 'Outcome' to 'diabetes'

```{r}

colnames(diab)[9] <- "diabetes"

```

## Reshape the datasets

**Diabetes? =\> 0 : No / 1 : Yes**

```{r}

diab$diabetes <- as.factor(diab$diabetes)

levels(diab$diabetes) <- c("No","Yes")

```

## View datasets {.tabset}

### structure

```{r}

str(diab)

```

### dimmension

768 rows, 9 columns

```{r}

dim(diab)

```

### head

```{r}

library(knitr)

kable(head(diab))

```

------------------------------------------------------------------------

# Analyze the Correlation between variables

## Correlation between each variables

```{r}

library(PerformanceAnalytics)

chart.Correlation(diab[,-9], histogram=TRUE, col="grey10", pch=1, main="Chart Correlation of Variance")

```

## See the relation between each variables (diabetes included)

By ggpairs, the figure **Pregnancies, Glucose, Age** seems different according to diabetes outcome.

###### {r}

library(ggplot2)

library(GGally)

ggpairs(diab, aes(color=diabetes, alpha=0.75), lower=list(continuous="smooth"))+ theme_bw()+

labs(title="Correlation Plot of Variance(diabetes)")+

theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=12))

## See the ggcorr plot

By ggcorr, we can see high correlation in below variance

-   Pregnancies & Age : 0.5 =\> About 50% correlated to each other

-   SkinThickness - &Insulin, &BMI : 0.4 =\> About 40% correlated to each other

```{r}
library(ggplot2)

library(GGally)

ggpairs(diab, aes(color=diabetes, alpha=0.75), lower=list(continuous="smooth"))+ theme_bw()+
labs(title="Correlation Plot of Variance(diabetes)")+
theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=12))


ggcorr(diab[,-9], name = "corr", label = TRUE)+
  theme(legend.position="none")+
  labs(title="Correlation Plot of Variance")+
  theme(plot.title=element_text(face='bold',color='black',hjust=0.5,size=12))

```

------------------------------------------------------------------------

# Apply ML methods

## Make test & train dataset

Shuffle the diab data(100%) & Make train dataset(70%), test dataset(30%)

```{r}

nrows <- NROW(diab)

set.seed(218)				            # fix random value

index <- sample(1:nrows, 0.7 * nrows)	# shuffle and divide



# train <- diab                         # 768 test data (100%)

train <- diab[index,]			        # 537 test data (70%)

test <- diab[-index,]  		            # 231 test data (30%)

```

## Check the proportion of diabetes (Benign / Malignant) {.tabset}

### train

```{r}

prop.table(table(train$diabetes))

```

### test

```{r}

prop.table(table(test$diabetes))

```

## Apply every ML methods(that I know) to data {.tabset}

```{r}

# This library is for confusionMatrix

library(caret)

```

### C5.0 - Tune

#### Choose 'trials' which shows best predict performance in C5.0

```{r}

library(C50)



acc_test <- numeric()

accuracy1 <- NULL; accuracy2 <- NULL



for(i in 1:50){

    learn_imp_c50 <- C5.0(train[,-9],train$diabetes,trials = i)      

    p_c50 <- predict(learn_imp_c50, test[,-9]) 

    accuracy1 <- confusionMatrix(p_c50, test$diabetes)

    accuracy2[i] <- accuracy1$overall[1]

}



acc <- data.frame(t= seq(1,50), cnt = accuracy2)



opt_t <- subset(acc, cnt==max(cnt))[1,]

sub <- paste("Optimal number of trials is", opt_t$t, "(accuracy :", opt_t$cnt,") in C5.0")



library(highcharter)

hchart(acc, 'line', hcaes(t, cnt)) %>%

  hc_title(text = "Accuracy With Varying Trials (C5.0)") %>%

  hc_subtitle(text = sub) %>%

  hc_add_theme(hc_theme_google()) %>%

  hc_xAxis(title = list(text = "Number of Trials")) %>%

  hc_yAxis(title = list(text = "Accuracy"))





```

#### Apply optimal trials to show best predict performance in C5.0

```{r}

learn_imp_c50 <- C5.0(train[,-9],train$diabetes,trials=opt_t$t)	

pre_imp_c50 <- predict(learn_imp_c50, test[,-9])

cm_imp_c50 <- confusionMatrix(pre_imp_c50, test$diabetes)

cm_imp_c50

```

### rpart

```{r}

library(rpart)

learn_rp <- rpart(diabetes~.,data=train,control=rpart.control(minsplit=2))

pre_rp <- predict(learn_rp, test[,-9], type="class")

cm_rp  <- confusionMatrix(pre_rp, test$diabetes)	

cm_rp

```

### Prune

```{r}

learn_pru <- prune(learn_rp, cp=learn_rp$cptable[which.min(learn_rp$cptable[,"xerror"]),"CP"])

pre_pru <- predict(learn_pru, test[,-9], type="class")

cm_pru <-confusionMatrix(pre_pru, test$diabetes)			

cm_pru

```

### OneR

```{r}

#install.packages("RWeka")

library("RWeka")

learn_1r <- OneR(diabetes~., data=train)

pre_1r <- predict(learn_1r, test[,-9])

cm_1r   <- confusionMatrix(pre_1r, test$diabetes)

cm_1r

```

### JRip

```{r}

learn_jrip <- JRip(diabetes ~ ., data=train)

pre_jrip <- predict(learn_jrip, test[,-9])

cm_jrip <- confusionMatrix(pre_jrip, test$diabetes)		

cm_jrip

```

### naiveBayes

#### Choose 'laplace' which shows best predict performance in naiveBayes

It shows that "laplace" function is not effective to naiveBayes predict performance.

So, It's okay not to use laplace option for tuning.

```{r}

library(e1071)



acc_test <- numeric()

accuracy1 <- NULL; accuracy2 <- NULL



for(i in 1:30){

    learn_imp_nb <- naiveBayes(train[,-9], train$diabetes, laplace=i)    

    p_nb <- predict(learn_imp_nb, test[,-9]) 

    accuracy1 <- confusionMatrix(p_nb, test$diabetes)

    accuracy2[i] <- accuracy1$overall[1]

}



acc <- data.frame(l= seq(1,30), cnt = accuracy2)



opt_l <- subset(acc, cnt==max(cnt))[1,]

sub <- paste("Optimal number of laplace is", opt_l$l, "(accuracy :", opt_l$cnt,") in naiveBayes")



library(highcharter)

hchart(acc, 'line', hcaes(l, cnt)) %>%

  hc_title(text = "Accuracy With Varying Laplace (naiveBayes)") %>%

  hc_subtitle(text = sub) %>%

  hc_add_theme(hc_theme_google()) %>%

  hc_xAxis(title = list(text = "Number of Laplace")) %>%

  hc_yAxis(title = list(text = "Accuracy"))



```

#### naiveBayes without laplace

```{r}

learn_nb <- naiveBayes(train[,-9], train$diabetes)

pre_nb <- predict(learn_nb, test[,-9])

cm_nb <- confusionMatrix(pre_nb, test$diabetes)		

cm_nb

```

### randomForest

```{r}

library(randomForest)

learn_rf <- randomForest(diabetes~., data=train, ntree=500, proximity=T, importance=T)

pre_rf   <- predict(learn_rf, test[,-9])

cm_rf    <- confusionMatrix(pre_rf, test$diabetes)

cm_rf

```

```{r}

plot(learn_rf, main="Random Forest (Error Rate vs. Number of Trees)")

```

#### Prediction Plot

I can't explain this plot exactly.

Anybody who can describe this plot, please let me know. I'm happy to add in my kernel.

```{r}

plot(margin(learn_rf,test$diabetes))

```

##### Variance Importance Plot

-   MeanDecreaseAccuracy : radius_worst \> concave.points_worst \> area_worst \> perimeter_worst

Important parameters for accuracy improvement are determined by the "MeanDecreaseAccuracy".

-   MeanDecreaseGini : perimeter_worst \> radius_worst \> area_worst \> concave.points_worst

Important parameters for improving node impurities are determined by the "MeanDecreaseGini".

```{r}

varImpPlot(learn_rf)

```

### ctree

```{r}

library(party)

learn_ct <- ctree(diabetes~., data=train, controls=ctree_control(maxdepth=2))

pre_ct   <- predict(learn_ct, test[,-9])

cm_ct    <- confusionMatrix(pre_ct, test$diabetes)

cm_ct

```

### KNN - Tune

#### Choose 'k' which shows best predict performance in KNN

```{r}

library(class)



acc_test <- numeric() 



for(i in 1:30){

    predict <- knn(train=train[,-9], test=test[,-9], cl=train[,9], k=i, prob=T)

    acc_test <- c(acc_test,mean(predict==test[,9]))

}



acc <- data.frame(k= seq(1,30), cnt = acc_test)



opt_k <- subset(acc, cnt==max(cnt))[1,]

sub <- paste("Optimal number of k is", opt_k$k, "(accuracy :", opt_k$cnt,") in KNN")



library(highcharter)

hchart(acc, 'line', hcaes(k, cnt)) %>%

  hc_title(text = "Accuracy With Varying K (KNN)") %>%

  hc_subtitle(text = sub) %>%

  hc_add_theme(hc_theme_google()) %>%

  hc_xAxis(title = list(text = "Number of Neighbors(k)")) %>%

  hc_yAxis(title = list(text = "Accuracy"))



  

```

#### Apply optimal K to show best predict performance in KNN

```{r}

pre_knn <- knn(train = train[,-9], test = test[,-9], cl = train[,9], k=opt_k$k, prob=T)

cm_knn  <- confusionMatrix(pre_knn, test$diabetes)

cm_knn

```

### GBM

```{r}

library(gbm)

test_gbm <- gbm(diabetes~., data=train, distribution="gaussian", n.trees = 10000,

                shrinkage = 0.01, interaction.depth = 4, bag.fraction=0.5, train.fraction=0.5,n.minobsinnode=10,cv.folds=3,keep.data=TRUE,verbose=FALSE,n.cores=1)

best.iter <- gbm.perf(test_gbm, method="cv",plot.it=FALSE)

fitControl = trainControl(method="cv", number=5, returnResamp="all")

learn_gbm = train(diabetes~., data=train, method="gbm", distribution="bernoulli", trControl=fitControl, verbose=F, tuneGrid=data.frame(.n.trees=best.iter, .shrinkage=0.01, .interaction.depth=1, .n.minobsinnode=1))

pre_gbm <- predict(learn_gbm, test[,-9])

cm_gbm <- confusionMatrix(pre_gbm, test$diabetes)

cm_gbm

```

### adaBoost

```{r}

library(rpart)

library(ada)

control <- rpart.control(cp = -1, maxdepth = 14,maxcompete = 1,xval = 0)

learn_ada <- ada(diabetes~., data = train, test.x = train[,-9], test.y = train[,9], type = "gentle", control = control, iter = 70)

pre_ada <- predict(learn_ada, test[,-9])

cm_ada <- confusionMatrix(pre_ada, test$diabetes)

cm_ada

```

### SVM - Tune

#### Choose 'gamma, cost' which shows best predict performance in SVM

```{r}

library(e1071)

gamma <- seq(0,0.1,0.005)

cost <- 2^(0:5)

parms <- expand.grid(cost=cost, gamma=gamma)    # 231



acc_test <- numeric()

accuracy1 <- NULL; accuracy2 <- NULL



for(i in 1:NROW(parms)){        

        learn_svm <- svm(diabetes~., data=train, gamma=parms$gamma[i], cost=parms$cost[i])

        pre_svm <- predict(learn_svm, test[,-9])

        accuracy1 <- confusionMatrix(pre_svm, test$diabetes)

        accuracy2[i] <- accuracy1$overall[1]

}



acc <- data.frame(p= seq(1,NROW(parms)), cnt = accuracy2)



opt_p <- subset(acc, cnt==max(cnt))[1,]

sub <- paste("Optimal number of parameter is", opt_p$p, "(accuracy :", opt_p$cnt,") in SVM")



library(highcharter)

hchart(acc, 'line', hcaes(p, cnt)) %>%

  hc_title(text = "Accuracy With Varying Parameters (SVM)") %>%

  hc_subtitle(text = sub) %>%

  hc_add_theme(hc_theme_google()) %>%

  hc_xAxis(title = list(text = "Number of Parameters")) %>%

  hc_yAxis(title = list(text = "Accuracy"))

  

```

#### Show best gamma and cost values

```{r}

print(paste("Best Cost :",parms$cost[opt_p$p],", Best Gamma:",parms$gamma[opt_p$p]))

```

#### Apply optimal parameters(gamma, cost) to show best predict performance in SVM

```{r}

learn_imp_svm <- svm(diabetes~., data=train, cost=parms$cost[opt_p$p], gamma=parms$gamma[opt_p$p])

pre_imp_svm <- predict(learn_imp_svm, test[,-9])

cm_imp_svm <- confusionMatrix(pre_imp_svm, test$diabetes)

cm_imp_svm

```

## Total Summary & Choose Best ML

### Visualize to compare the accuracy of all methods

```{r}

col <- c("#ed3b3b", "#0099ff")

par(mfrow=c(3,4))

fourfoldplot(cm_imp_c50$table, color = col, conf.level = 0, margin = 1, main=paste("Tune C5.0 (",round(cm_imp_c50$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_rp$table, color = col, conf.level = 0, margin = 1, main=paste("RPart (",round(cm_rp$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_pru$table, color = col, conf.level = 0, margin = 1, main=paste("Prune (",round(cm_pru$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_1r$table, color = col, conf.level = 0, margin = 1, main=paste("OneR (",round(cm_1r$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_jrip$table, color = col, conf.level = 0, margin = 1, main=paste("JRip (",round(cm_jrip$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_ct$table, color = col, conf.level = 0, margin = 1, main=paste("CTree (",round(cm_ct$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_nb$table, color = col, conf.level = 0, margin = 1, main=paste("NaiveBayes (",round(cm_nb$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_knn$table, color = col, conf.level = 0, margin = 1, main=paste("Tune KNN (",round(cm_knn$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_rf$table, color = col, conf.level = 0, margin = 1, main=paste("RandomForest (",round(cm_rf$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_gbm$table, color = col, conf.level = 0, margin = 1, main=paste("GBM (",round(cm_gbm$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_ada$table, color = col, conf.level = 0, margin = 1, main=paste("AdaBoost (",round(cm_ada$overall[1]*100),"%)",sep=""))

fourfoldplot(cm_imp_svm$table, color = col, conf.level = 0, margin = 1, main=paste("Tune SVM (",round(cm_imp_svm$overall[1]*100),"%)",sep=""))

```

### Select a best prediction model according to high accuracy

```{r}

opt_predict <- c(cm_imp_c50$overall[1], cm_rp$overall[1], cm_pru$overall[1], cm_1r$overall[1], cm_jrip$overall[1], cm_ct$overall[1], cm_nb$overall[1], cm_knn$overall[1], cm_rf$overall[1], cm_gbm$overall[1], cm_ada$overall[1], cm_imp_svm$overall[1])

names(opt_predict) <- c("tune_c50","rpart","prune","1r","jrip","ctree","nb","tune_knn","rf","gbm","ada","tune_svm")

best_predict_model <- subset(opt_predict, opt_predict==max(opt_predict))

best_predict_model

```

------------------------------------------------------------------------

# Test Dataset Prediction

## Predict 1 patient data's Diabetes

### Prepare Patient data for testing function

If you want to make your own new data, make sure your data format is same as below.

#### Import patient data {.tabset}

Pick one row just for testing!

-   Why one? I thought that in real life, it is more common to diagnosis only one patient at once.

##### Diabetes: Yes

```{r}

Y <- test[1,]                   ## 5th patient

kable(Y)                        ## Diabetes: Yes

```

##### Diabetes: No

```{r}

N <- test[2,]                   ## 18th patient          

kable(N)                        ## Diabetes: No

```

#### Delete **diabetes** column for testing

```{r}

Y$diabetes <- NULL

N$diabetes <- NULL

```

### Patient Diabetes Function

#### Make Function

```{r}

patient_diabetes_predict <- function(new, method=learn_imp_svm) {

    new_pre <- predict(method, new)

    new_res <- as.character(new_pre)

    return(paste("Result: ", new_res, sep=""))

}

```

#### Testing Function {.tabset}

-   Use 'Tuned SVM Algorithm' as default, Since it's rated as the best predict_model.

-   But not always best_predict_model is good. I think in real situation, it's more important to reduce the (diabetes: Yes -\> Predict: No) faulty prediction.

-   So I think tuned c5.0 model which shows the lowest rate(38) is the best in choosing model for prediction.

-   You can compare each models which is more correct.

##### Diabetes: Yes

-   default = improve svm

```{r}

patient_diabetes_predict(Y) 

```

-   Use other ML methods

```{r}

patient_diabetes_predict(Y,learn_imp_c50)

```

##### Diabetes: No

-   default = improve svm

```{r}

patient_diabetes_predict(N) 

```

-   Use other ML methods

```{r}

patient_diabetes_predict(N,learn_imp_c50)

```

## Predict Test dataset's Diabetes

-   Using Tuned C5.0 model

```{r}

sub <- data.frame(orgin_result = test$diabetes, predict_result = pre_imp_c50, correct = ifelse(test$diabetes == pre_imp_c50, "True", "False"))

kable(head(sub,10))

prop.table(table(sub$correct))

```

------------------------------------------------------------------------

# Visualize (Probabilty Density Function Graph)

I made this plot for doctors who diagnosis diabetes for patients.

From the patient's point of view, I visualized the diabetes results in probability density graph with **patients diagnosis strong line** included, so that they can check their status at once.

If patient's factor of diabetes is above **diabetes:yes** factor average, I colored it with red line.

## Create Visualize Function

```{r visual1}

diabetes_summary <- function(new,data) {



## [a] Reshape the new dataset for ggplot

library(reshape2)

m_train <- melt(data, id="diabetes")

m_new <- melt(new)





## [b] Save mean of Malignant value

library(dplyr)

mal_mean <- subset(data, diabetes == "Yes", select = -9)

mal_mean <- apply(mal_mean, 2, mean)



## [c] highlight with red colors line

library(stringr)

mal_col <- ifelse((round(m_new$value,3) > mal_mean), "red", "black")







## [d] Save titles : Main title, Patient Diagnosis



title <- paste("Diabetes Diagnosis Plot (", patient_diabetes_predict(new),")",sep="")





## ★[f] View plots highlighting values above average of malignant patient

res_mean <- ggplot(m_train, aes(x=value,color=diabetes, fill=diabetes))+

    geom_histogram(aes(y=..density..), alpha=0.5, position="identity", bins=50)+

    geom_density(alpha=.2)+

    scale_color_manual(values=c("#15c3c9","#f87b72"))+

    scale_fill_manual(values=c("#61d4d6","#f5a7a1"))+

    geom_vline(data=m_new, aes(xintercept=value), 

               color=mal_col, size=1.5)+

    geom_label(data=m_new, aes(x=Inf, y=Inf, label=round(value,3)), nudge_y=2,  

               vjust = "top", hjust = "right", fill="white", color="black")+

    labs(title=title)+

    theme(plot.title = element_text(face='bold', colour='black', hjust=0.5, size=15))+

    theme(plot.subtitle=element_text(lineheight=0.8, hjust=0.5, size=12))+

    labs(caption="[Training 537 Pima Indians Diabetes Data]")+

    facet_wrap(~variable, scales="free", ncol=4)







## [g] output graph

res_mean



}

```

## Testing Function {.tabset}

### Diabetes

```{r fig.width=10, fig.height=6, fig.align='center'}

diabetes_summary(Y, diab)

```

### Normal

```{r fig.width=10, fig.height=6, fig.align='center'}

diabetes_summary(N, diab)

```

------------------------------------------------------------------------

# End

Upvotes and Comments are fully Welcomed :-)

Want to see my another kernels?

-   **Classification** orginal kernel, click [here: Breast Cancer or Not (with 15 ML)](https://www.kaggle.com/mirichoi0218/classification-breast-cancer-or-not-with-15-ml)

-   **Regression**, click [here: How much will the premium be?](https://www.kaggle.com/mirichoi0218/regression-how-much-will-the-premium-be)

Thank you for watching!
